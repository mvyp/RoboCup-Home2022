运行深度相机的那个roslaunch文件
运行try.py  # 这是会调用ros
try_test_camera.py 也行 这是调用笔记本电脑做测试的

我把那个main函数重命名为 human_tools.py 
然后我把程序新写到try.py 里面
并在try.py去调用human_tools里面的函数，
这样你们如果要修改一些参数就直接修改human_tools,就好

我对human_tools 做了一些细小改动。 haarcascade_frontalface_alt.xml 这个东西要和human_tools
放在同级目录下



百度这个api挺看网速的，用了百度api不一定能做到实时

我把逻辑改了一点：
原本是 沙发是否有人检测，然后不管有无人都 还再用百度api去看这个人的具体信息

改为
    1. 沙发是否有人检测
    2. 没人的话就是继续检测
    3. 有人的话再调用百度api去看
    4. 因为原本是有些可视化的东西，但在ros上是无所谓的，可以先通过ros获得坐标信息再去可视化
        所以我小改了detect_show 和 human_show 函数里面的东西, 以及判断是否有人在Sofa的判断方法.

最后将通过ros节点 /human_state_detect 发布一些信息

格式如下：
是否有人-座位标志位seat_nmm-人坐标x-人坐标y-人w-人h
比如:1-1-245-142-202-202, 0-0-0-0-0-0

是否有人： 无人0, 有人则是一个大于0的整数，一般为1  # 这个的用来判断画面中是否有人脸
座位标志位seat_nmm：和detect_show函数中你们写的判断逻辑是一样的意义，0或1, 你们写的这个意思应该是 沙发上是否有人
人坐标x-人坐标y-人w-人h： 如果有人的话，这个里的坐标就是实际检测出来的坐标，比如 245-142-202-202
                        如果没有人的话就是 0-0-0-0
